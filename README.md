# NYC Taxi Streaming Pipeline ðŸš•ðŸ“Š

A real-time data engineering project that streams **NYC Taxi Trip data** from a producer into **Kafka**, processes it with **PyFlink**, and stores the transformed results into **PostgreSQL** for downstream analytics.  

---

## ðŸ“Œ Project Overview
This project simulates a **real-time streaming pipeline**:
1. **Producer (Python)**  
   Reads NYC Taxi trip data (Parquet/CSV) and sends events into **Kafka**.
2. **Kafka**  
   Acts as the **message broker** to handle real-time event streams.
3. **PyFlink**  
   Consumes events from Kafka, applies simple **transformations**, and prepares data for storage.
4. **PostgreSQL**  
   Stores processed trip records for analytics, BI dashboards, and reporting.

---

## âš™ï¸ Tech Stack
- **Apache Kafka** â€“ Event streaming platform  
- **Apache Flink (PyFlink)** â€“ Real-time stream processing  
- **PostgreSQL** â€“ Analytical database  
- **Docker Compose** â€“ Container orchestration  
- **Python** â€“ Data producer & ETL logic  

---

## ðŸš€ Getting Started

### 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/your-username/nyc-taxi-streaming-pipeline.git
cd nyc-taxi-streaming-pipeline
  end

## ðŸ—ï¸ Architecture

```mermaid
flowchart TD
    A["NYC Taxi Data\n(CSV / Parquet)"] -->|Producer.py| B[Kafka]
    B --> C["PyFlink\nStream Processing"]
    C --> D["PostgreSQL\nTaxi Events Table"]
    D --> E["BI / Analytics Tools"]





